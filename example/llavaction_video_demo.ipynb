{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLaVAction: Evaluating and Training Multi-Modal Large Language Models for Action Recognition\n",
        "\n",
        "- This repository contains the implementation for our preprint on evaluating and training multi-modal large language models for action recognition.\n",
        "- ‚ö†Ô∏è The demo *requires* an NVIDIA GPU, such as an A100.\n",
        "\n"
      ],
      "metadata": {
        "id": "moPRHYOWkOKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab our model weights and demo data:\n",
        "- Installation and data/model downloading on Google Colab (or locally) should take roughly ~10 min."
      ],
      "metadata": {
        "id": "gTr6BFHuva0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"MLAdaptiveIntelligence/LLaVAction-0.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"MLAdaptiveIntelligence/LLaVAction-0.5B\")"
      ],
      "metadata": {
        "id": "SXNjH-9yAUft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### demo data:"
      ],
      "metadata": {
        "id": "AYF67ETG_njy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --folder https://drive.google.com/drive/folders/1ql8MSWTK-2_uGH1EzPOrifauwUNg4E6i -O ./data"
      ],
      "metadata": {
        "id": "qMMuLNIEvbEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data\n",
        "%mkdir '/content/data/code'\n",
        "%ls"
      ],
      "metadata": {
        "id": "lTH58_HlwYOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö® Install our LLaVAction source code:\n"
      ],
      "metadata": {
        "id": "YL6WxyNmz_Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -pre llavaction"
      ],
      "metadata": {
        "id": "mGPb3kp4_zB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install additional code dependencies\n",
        "\n",
        "- Installing flash attention, which is important for fast inference.\n",
        "- Install decord for efficient video reading.\n",
        "- Installing LLaVAction source code.\n"
      ],
      "metadata": {
        "id": "rot5HYWoHoBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install decord"
      ],
      "metadata": {
        "id": "zMgB6_Kkv26W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import inference and visualization functions from LLaVAction"
      ],
      "metadata": {
        "id": "e6SsiJzDIJt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import llavaction\n",
        "from llavaction.action.selective_inference import SelectiveInferencer\n",
        "from llavaction.action.make_visualizations import visualize_with_uid\n",
        "import os"
      ],
      "metadata": {
        "id": "tK7pnU99qJzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Key parameters for LLaVAction:\n",
        "\n",
        "- Specify where to load the EPIC-KITCHENS-100 videos and the LLaVAction checkpoint for the inference. If you followed our steps above, these paths do not need changed for Google Colaboratory.\n",
        "- ‚ö†Ô∏è You can adjust `n_frames` to a higher number for better performance (which we empirically observed), but note this uses more compute."
      ],
      "metadata": {
        "id": "wvopr62aIM94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = '/content/data/EK100_512/EK100'\n",
        "checkpoint_folder = '/content/data/checkpoint/dev_ov_0.5b_16f_top5_full'\n",
        "n_frames = 16\n",
        "\n",
        "inferencer = SelectiveInferencer(data_root,\n",
        "                                     checkpoint_folder,\n",
        "                                     include_time_instruction = False,\n",
        "                                     n_frames = n_frames,\n",
        "                                    use_flash_attention = True)"
      ],
      "metadata": {
        "id": "HntA8BHGqRb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the 'caption' mode of the inference."
      ],
      "metadata": {
        "id": "C0BPzu3PIRIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note, this does not need adjusted for the demo\n",
        "def get_caption(inferencer,\n",
        "                uid,\n",
        "                checkpoint_folder):\n",
        "    caption =  inferencer.inference('',\n",
        "                                     uid,\n",
        "                                     'caption')\n",
        "    return caption"
      ],
      "metadata": {
        "id": "qp0xYBYZvFgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You can modify the time range to query LLaVAction\n",
        "\n",
        "- Our demo video is called `P01-P01_01` represents the video ID.\n",
        "- `3.00` and `4.00` denotes the start and end in seconds, which you can modify (up to 15 seconds)."
      ],
      "metadata": {
        "id": "OW0bOMPrITT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can change the start and stop time:\n",
        "# for the demo video we provide, it is 15 seconds long:\n",
        "start = '3.00'\n",
        "stop = '4.00'\n",
        "\n",
        "#video ID:\n",
        "uid = f'P01-P01_01_{start}_{stop}'"
      ],
      "metadata": {
        "id": "Y6j-ZiQedRgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the video frames in the specified time range"
      ],
      "metadata": {
        "id": "EyDkk5jHgc2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_with_uid(data_root, uid, 'vis_folder')\n",
        "\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "folder_path = f\"vis_folder/{uid}\"\n",
        "\n",
        "\n",
        "# List all image files\n",
        "image_files = sorted([f for f in os.listdir(folder_path) if f.endswith((\".jpg\", \".png\", \".jpeg\"))])\n",
        "\n",
        "# Set grid dimensions\n",
        "cols = 4  # Adjust this for the number of images per row\n",
        "rows = (len(image_files) + cols - 1) // cols  # Calculate the required number of rows\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))  # Adjust figure size\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.05)  # Reduce horizontal & vertical spacing\n",
        "\n",
        "# Loop through images and display them in the grid\n",
        "for ax, img_file in zip(axes.flatten(), image_files):\n",
        "    img_path = os.path.join(folder_path, img_file)\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for proper display\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(img_file, fontsize=8)  # Display filename in smaller font\n",
        "    ax.axis(\"off\")  # Hide axis labels\n",
        "\n",
        "# Hide unused subplots (if any)\n",
        "for ax in axes.flatten()[len(image_files):]:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNXNoBmmyeCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the caption inference using LLaVAction\n",
        "- this runs  on the video with the specified timestamps set above!"
      ],
      "metadata": {
        "id": "w5IQMfSYIXHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_caption = get_caption(inferencer, uid, checkpoint_folder)\n",
        "output_caption"
      ],
      "metadata": {
        "id": "8LhrLRGk8jTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Tasks to consider\n",
        "- here we use `spacy` do to some simple grouping of verbs/nouns."
      ],
      "metadata": {
        "id": "QkkbP8cjAB-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = output_caption\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract words with timestamps (sentence index)\n",
        "nouns, verbs = [], []\n",
        "for i, sent in enumerate(doc.sents):\n",
        "    for token in sent:\n",
        "        if token.pos_ in [\"NOUN\"]:\n",
        "            nouns.append((i, token.text))\n",
        "        elif token.pos_ in [\"VERB\"]:\n",
        "            verbs.append((i, token.text))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, (t, word) in enumerate(nouns):\n",
        "    plt.scatter(t, i, color='blue', label=\"Noun\" if i == 0 else \"\", marker='o')\n",
        "    plt.text(t, i, word, fontsize=12, verticalalignment='bottom')\n",
        "\n",
        "for i, (t, word) in enumerate(verbs):\n",
        "    plt.scatter(t, i + len(nouns), color='red', label=\"Verb\" if i == 0 else \"\", marker='x')\n",
        "    plt.text(t, i + len(nouns), word, fontsize=12, verticalalignment='bottom')\n",
        "\n",
        "plt.xlabel(\"Time (Sentence Index)\")\n",
        "plt.ylabel(\"Word Occurrence\")\n",
        "plt.title(\"Ethogram of Nouns and Verbs\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bMz3ga7DhRTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count nouns and verbs\n",
        "num_nouns = len(nouns)\n",
        "num_verbs = len(verbs)"
      ],
      "metadata": {
        "id": "F0y6esiVh_Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count occurrences of each noun and verb\n",
        "noun_counts = Counter([word for _, word in nouns])\n",
        "verb_counts = Counter([word for _, word in verbs])\n",
        "\n",
        "# Plot pie charts\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Noun pie chart\n",
        "axes[0].pie(noun_counts.values(), labels=noun_counts.keys(), autopct=\"%1.1f%%\", colors=plt.cm.Blues_r(range(len(noun_counts))))\n",
        "axes[0].set_title(\"Noun Distribution\")\n",
        "\n",
        "# Verb pie chart\n",
        "axes[1].pie(verb_counts.values(), labels=verb_counts.keys(), autopct=\"%1.1f%%\", colors=plt.cm.Reds_r(range(len(verb_counts))))\n",
        "axes[1].set_title(\"Verb Distribution\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZYKCp0d8iHbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}