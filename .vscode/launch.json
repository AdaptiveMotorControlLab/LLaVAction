{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Run LLAVA Training with torchrun",
            "type": "debugpy",
            "request": "launch",
            "module": "torch.distributed.run",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1,2,3",
                "OMP_NUM_THREADS": "8",
                "NCCL_IB_DISABLE": "0",
                "NCCL_IB_GID_INDEX": "3",
                "NCCL_SOCKET_IFNAME": "eth0",
                "NCCL_DEBUG": "INFO",
                "ACCELERATE_CPU_AFFINITY": "1",
                "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7",
                "WANDB_API_KEY": "65aeda82a75f1eed29c8e9250b175fcc73dca0d7",
                "CUDA_LAUNCH_BLOCKING": "1",
            },
            "args": [
                "--nproc_per_node=4",
                "--nnodes=1",
                "--node_rank=0",
                "--master_addr=127.0.0.1",
                "--master_port=29500",
                "llava/train/train_mem.py",
                "--deepspeed", "scripts/zero3.json",
                "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
                "--version", "qwen_1_5",
                "--data_path", "scripts/train/onevision.yaml",
                // "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data",
                "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data/geo3k/",
                "--video_folder", "/mediaPFM/data/haozhe/onevision/llava_video",
                // "--video_folder", "/home/haozhe/kitchen/AVION/datasets",
                "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
                "--mm_vision_tower_lr", "2e-6",
                "--vision_tower", "google/siglip-so400m-patch14-384",
                "--mm_projector_type", "mlp2x_gelu",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                "--group_by_modality_length", "True",
                "--image_aspect_ratio", "anyres_max_9",
                "--image_grid_pinpoints", "(1x1),...,(6x6)",
                "--mm_patch_merge_type", "spatial_unpad",
                "--bf16", "True",
                "--run_name", "test1",
                "--output_dir", "experiments/test1",
                "--num_train_epochs", "1",
                "--per_device_train_batch_size", "1",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps", "2",
                "--evaluation_strategy", "steps",
                "--eval_steps", "10",
                "--save_strategy", "steps",
                "--save_steps", "2000",
                // "--save_total_limit", "1",
                "--learning_rate", "1e-5",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                "--tf32", "True",
                "--model_max_length", "32768",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "4",
                "--lazy_preprocess", "True",
                "--report_to", "wandb",
                "--torch_compile", "True",
                "--torch_compile_backend", "inductor",
                "--dataloader_drop_last", "True",
                "--frames_upbound", "16",
                "--root", "/mediaPFM/data/haozhe/onevision/llava_video/EK100",
                "--action_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_val.json",
                "--val_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_validation.csv",
                "--llava_num_frames", "16",
                "--clip_length", "16",
                "--action_representation", "GT_random_narration",
                "--topk_predictions", "5",
                "--dataset", "ek100_cls",
                "--vision_supervision", "newline",
                "--action_types", "97,300,3806"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}"
        }
    ]
}


// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "docs/LLaVA_OneVision_Tutorials.py",
//             "console": "integratedTerminal",
//             "env":{
//                 "CUDA_VISIBLE_DEVICES":"0,1,2,3",
//                 "HF_HOME": "huggingface",
//                 // "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7"
//                 },
//             "justMyCode": false,
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "docs/LLaVA_OneVision_debug.py",
//             "console": "integratedTerminal",
//             "env":{
//                 "CUDA_VISIBLE_DEVICES":"0,1,2,3",
//                 "HF_HOME": "huggingface",
//                 // "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7"
//                 },
//             "args": [
//                 "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
//                 "--version", "qwen_1_5",
//                 "--data_path", "scripts/train/onevision.yaml",
//                 // "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data",
//                 "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data/geo3k/",
//                 "--video_folder", "/mediaPFM/data/haozhe/onevision/llava_video",
//                 // "--video_folder", "/home/haozhe/kitchen/AVION/datasets",
//                 "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
//                 "--mm_vision_tower_lr", "2e-6",
//                 "--vision_tower", "google/siglip-so400m-patch14-384",
//                 "--mm_projector_type", "mlp2x_gelu",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end", "False",
//                 "--mm_use_im_patch_token", "False",
//                 "--group_by_modality_length", "True",
//                 "--image_aspect_ratio", "anyres_max_9",
//                 "--image_grid_pinpoints", "(1x1),...,(6x6)",
//                 "--mm_patch_merge_type", "spatial_unpad",
//                 "--bf16", "True",
//                 "--run_name", "test1",
//                 "--output_dir", "experiments/test1",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "1",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "2",
//                 "--evaluation_strategy", "steps",
//                 "--eval_steps", "10",
//                 "--save_strategy", "steps",
//                 "--save_steps", "2000",
//                 // "--save_total_limit", "1",
//                 "--learning_rate", "1e-5",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "32768",
//                 "--gradient_checkpointing", "True",
//                 "--dataloader_num_workers", "4",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "wandb",
//                 "--torch_compile", "True",
//                 "--torch_compile_backend", "inductor",
//                 "--dataloader_drop_last", "True",
//                 "--frames_upbound", "16",
//                 "--root", "/mediaPFM/data/haozhe/onevision/llava_video/EK100",
//                 "--action_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_val.json",
//                 "--val_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                 "--llava_num_frames", "16",
//                 "--clip_length", "16",
//                 "--action_representation", "GT_random_narration",
//                 "--topk_predictions", "5",
//                 "--dataset", "ek100_cls",
//                 "--vision_supervision", "newline",
//                 "--action_types", "97,300,3806"
//             ],
//             "justMyCode": false,
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "llava/action/generate_description.py",
//             "console": "integratedTerminal",
//             "env":{"CUDA_VISIBLE_DEVICES":"0"},
//             "justMyCode": false,
//             "args": [
//                 "--train_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_train.csv",
//                 "--out_folder", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA",
//                 "--train_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_train.json",
//                 "--gen_type", "avion_mc",
//                 "--n_options", "5",
//                 "--action_representation", "GT_key", //"GT_random_narration",
//                 "--n_narrations", "-1",
//             ]
//         }
//     ]
// }