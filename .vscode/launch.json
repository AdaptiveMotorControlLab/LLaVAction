{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Run LLAVA Training with torchrun",
            "type": "debugpy",
            "request": "launch",
            "module": "torch.distributed.run",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1,2,3",
                "OMP_NUM_THREADS": "8",
                "NCCL_IB_DISABLE": "0",
                "NCCL_IB_GID_INDEX": "3",
                "NCCL_SOCKET_IFNAME": "eth0",
                "NCCL_DEBUG": "INFO",
                "ACCELERATE_CPU_AFFINITY": "1",
                "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7",
                "WANDB_API_KEY": "65aeda82a75f1eed29c8e9250b175fcc73dca0d7",
                "CUDA_LAUNCH_BLOCKING": "1",
                "HF_HOME": "/media/data/haozhe/VFM/huggingface",
            },
            "args": [
                "--nproc_per_node=4",
                "--nnodes=1",
                "llava/train/train_mem.py",
                "--deepspeed", "scripts/zero3.json",
                "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
                "--version", "qwen_1_5",
                "--data_path", "scripts/train/llava_video_RCP.yaml",
                "--video_folder", "/media/data/haozhe/VFM/onevision/llava_video",
                "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
                "--mm_vision_tower_lr", "2e-6",
                "--vision_tower", "google/siglip-so400m-patch14-384",
                "--mm_projector_type", "mlp2x_gelu",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                "--group_by_modality_length", "True",
                "--image_aspect_ratio", "anyres_max_9",
                "--image_grid_pinpoints", "(1x1),...,(6x6)",
                "--mm_patch_merge_type", "spatial_unpad",
                "--bf16", "True",
                "--run_name", "dev_0.5b_llavavideo_haozhe",
                "--output_dir", "experiments/dev_0.5b_llavavideo_haozhe",
                "--num_train_epochs", "1",
                "--per_device_train_batch_size", "1",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps", "2",
                "--evaluation_strategy", "epoch",
                "--eval_steps", "1",
                "--save_strategy", "steps",
                "--save_steps", "2000",
                "--learning_rate", "1e-5",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                "--tf32", "True",
                "--model_max_length", "32768",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "4",
                "--lazy_preprocess", "True",
                "--report_to", "wandb",
                "--torch_compile", "True",
                "--torch_compile_backend", "inductor",
                "--dataloader_drop_last", "True",
                "--frames_upbound", "64",
                "--mm_newline_position", "grid",
                "--add_time_instruction", "True",
                "--force_sample", "True",
                "--mm_spatial_pool_stride", "2",
                "--root", "/media/data/haozhe/VFM/onevision/llava_video/EK100",
                "--action_predictions", "/media/data/haozhe/VFM/EK100/EK100_in_LLAVA/TIM/tim_pred_ids_val.json",
                "--val_metadata", "/media/data/haozhe/VFM/EK100/epic-kitchens-100-annotations/EPIC_100_validation.csv",
                "--llava_num_frames", "64",
                "--clip_length", "64",
                "--action_representation", "official_key",
                "--topk_predictions", "5"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}"
        }
    ]
}


// {
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Run LLAVA Training with torchrun",
//             "type": "debugpy",
//             "request": "launch",
//             "python": "/media/data/haozhe/VFM/llmseval-venv/bin/python",
//             "module": "accelerate.commands.launch",
//             "env": {
//                 "CUDA_VISIBLE_DEVICES": "0,1,2,3",
//                 "OMP_NUM_THREADS": "8",
//                 "NCCL_IB_DISABLE": "0",
//                 "NCCL_IB_GID_INDEX": "3",
//                 "NCCL_SOCKET_IFNAME": "eth0",
//                 "NCCL_DEBUG": "INFO",
//                 "ACCELERATE_CPU_AFFINITY": "1",
//                 "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7",
//                 "WANDB_API_KEY": "65aeda82a75f1eed29c8e9250b175fcc73dca0d7",
//                 "CUDA_LAUNCH_BLOCKING": "1",
//                 "HF_HOME": "/media/data/haozhe/VFM/huggingface",
//                 "OPENAI_API_KEY": "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"
//             },
//             "args": [
//                 "--num_processes", "4",
//                 "-m", "lmms_eval",
//                 // "--model", "llava_vid",
//                 "--model", "llava_onevision",
//                 // "--model_args", "pretrained=experiments/dev_LLaVA-Video-7B-Qwen2_4f_test_haozhe,conv_template=qwen_1_5,max_frames_num=64,mm_spatial_pool_mode=average",
//                 "--model_args", "pretrained=lmms-lab/llava-onevision-qwen2-0.5b-ov,conv_template=qwen_1_5,model_name=llava_qwen",
//                 "--tasks", "video_dc499",
//                 "--batch_size", "1",
//                 "--log_samples",
//                 "--log_samples_suffix", "llava_onevision",
//                 "--output_path", "./logs/"
//             ],
//             "console": "integratedTerminal",
//             "justMyCode": false,
//             "cwd": "${workspaceFolder}"
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "docs/LLaVA_OneVision_Tutorials.py",
//             "console": "integratedTerminal",
//             "env":{
//                 "CUDA_VISIBLE_DEVICES":"0,1,2,3",
//                 "HF_HOME": "huggingface",
//                 // "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7"
//                 },
//             "justMyCode": false,
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "docs/LLaVA_OneVision_debug.py",
//             "console": "integratedTerminal",
//             "env":{
//                 "CUDA_VISIBLE_DEVICES":"0,1,2,3",
//                 "HF_HOME": "huggingface",
//                 // "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7"
//                 },
//             "args": [
//                 "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
//                 "--version", "qwen_1_5",
//                 "--data_path", "scripts/train/onevision.yaml",
//                 // "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data",
//                 "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data/geo3k/",
//                 "--video_folder", "/mediaPFM/data/haozhe/onevision/llava_video",
//                 // "--video_folder", "/home/haozhe/kitchen/AVION/datasets",
//                 "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
//                 "--mm_vision_tower_lr", "2e-6",
//                 "--vision_tower", "google/siglip-so400m-patch14-384",
//                 "--mm_projector_type", "mlp2x_gelu",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end", "False",
//                 "--mm_use_im_patch_token", "False",
//                 "--group_by_modality_length", "True",
//                 "--image_aspect_ratio", "anyres_max_9",
//                 "--image_grid_pinpoints", "(1x1),...,(6x6)",
//                 "--mm_patch_merge_type", "spatial_unpad",
//                 "--bf16", "True",
//                 "--run_name", "test1",
//                 "--output_dir", "experiments/test1",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "1",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "2",
//                 "--evaluation_strategy", "steps",
//                 "--eval_steps", "10",
//                 "--save_strategy", "steps",
//                 "--save_steps", "2000",
//                 // "--save_total_limit", "1",
//                 "--learning_rate", "1e-5",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "32768",
//                 "--gradient_checkpointing", "True",
//                 "--dataloader_num_workers", "4",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "wandb",
//                 "--torch_compile", "True",
//                 "--torch_compile_backend", "inductor",
//                 "--dataloader_drop_last", "True",
//                 "--frames_upbound", "16",
//                 "--root", "/mediaPFM/data/haozhe/onevision/llava_video/EK100",
//                 "--action_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_val.json",
//                 "--val_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                 "--llava_num_frames", "16",
//                 "--clip_length", "16",
//                 "--action_representation", "GT_random_narration",
//                 "--topk_predictions", "5",
//                 "--dataset", "ek100_cls",
//                 "--vision_supervision", "newline",
//                 "--action_types", "97,300,3806"
//             ],
//             "justMyCode": false,
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "llava/action/generate_description.py",
//             "console": "integratedTerminal",
//             "env":{"CUDA_VISIBLE_DEVICES":"0"},
//             "justMyCode": false,
//             "args": [
//                 "--train_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_train.csv",
//                 "--out_folder", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA",
//                 "--train_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_train.json",
//                 "--gen_type", "avion_mc",
//                 "--n_options", "5",
//                 "--action_representation", "GT_key", //"GT_random_narration",
//                 "--n_narrations", "-1",
//             ]
//         }
//     ]
// }


// //shaokai's
// {
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Run LLAVA Training with torchrun",
//             "type": "debugpy",
//             "request": "launch",
//             "module": "torch.distributed.run",
//             "env": {
//                 "CUDA_VISIBLE_DEVICES": "0",
//                 "OMP_NUM_THREADS": "8",
//                 "NCCL_IB_DISABLE": "0",
//                 "NCCL_IB_GID_INDEX": "3",
//                 "NCCL_SOCKET_IFNAME": "eth0",
//                 "HF_HOME": "/data/shaokai",
//                 "NCCL_DEBUG": "INFO",
//                 "ACCELERATE_CPU_AFFINITY": "1",
//                 "WANDB_API_KEY": "4474ec79de023b0c3ffb43588ab6163264f875db",
//                 "PYTHONPATH": "/data/shaokai/LLaVA-NeXT:/usr/local/lib/python3.10/site-packages/decord-0.6.0-py3.10-linux-x86_64.egg/"
//             },
//             "args": [
//                 "--nproc_per_node=1",
//                 "--nnodes=1",
//                 "--node_rank=0",
//                 "--master_addr=127.0.0.1",
//                 "--master_port=29500",
//                 "llava/train/train_mem.py",                
//                 "--deepspeed", "scripts/zero3.json",
//                 "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
//                 "--version", "qwen_1_5",
//                 "--data_path", "scripts/train/simple_tim_top5_cut.yaml",
//                 "--video_folder", "/data/shaokai/",
//                 "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
//                 "--mm_vision_tower_lr", "2e-6",
//                 "--vision_tower", "google/siglip-so400m-patch14-384",
//                 "--mm_projector_type", "mlp2x_gelu",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end", "False",
//                 "--mm_use_im_patch_token", "False",
//                 "--group_by_modality_length", "True",
//                 "--image_aspect_ratio", "anyres_max_9",
//                 "--image_grid_pinpoints", "(1x1),...,(6x6)",
//                 "--mm_patch_merge_type", "spatial_unpad",
//                 "--bf16", "True",
//                 "--run_name", "dpo_test",
//                 "--output_dir", "experiments/dpo_test",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "1",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "2",
//                 "--evaluation_strategy", "steps",
//                 "--save_strategy", "steps",
//                 "--save_steps", "1000",
//                 "--save_total_limit", "1",
//                 "--learning_rate", "1e-5",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "32768",
//                 "--gradient_checkpointing", "True",
//                 "--dataloader_num_workers", "4",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "wandb",
//                 "--torch_compile", "True",
//                 "--torch_compile_backend", "inductor",
//                 "--dataloader_drop_last", "True",
//                 "--frames_upbound", "32",
//                 "--root", "/data/shaokai/EK100",
//                 "--action_predictions", "/data/shaokai/TIM_PREDS/tim_pred_ids_val.json",
//                 "--val_metadata", "/data/shaokai/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                 "--llava_num_frames", "16",
//                 "--clip_length", "16",
//                 "--action_representation", "official_key",
//                 "--topk_predictions", "10",
//                 "--eval_steps", "1",
//                 "--vision_supervision", "three_tokens",
//                 "--action_types", "97,300,3806",
//                 "--n_narration", "5"
//             ],
//             "console": "integratedTerminal",
//             "justMyCode": false,
//             "cwd": "${workspaceFolder}"
//         }
//     ]
// }


