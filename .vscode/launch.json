// {
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Run LLAVA Training with torchrun",
//             "type": "debugpy",
//             "request": "launch",
//             "module": "torch.distributed.run",
//             "env": {
//                 "CUDA_VISIBLE_DEVICES": "0,2,3",
//                 "OMP_NUM_THREADS": "8",
//                 "NCCL_IB_DISABLE": "0",
//                 "NCCL_IB_GID_INDEX": "3",
//                 "NCCL_SOCKET_IFNAME": "eth0",
//                 "NCCL_DEBUG": "INFO",
//                 "ACCELERATE_CPU_AFFINITY": "1",
//                 "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7",
//                 "WANDB_API_KEY": "65aeda82a75f1eed29c8e9250b175fcc73dca0d7",
//                 "CUDA_LAUNCH_BLOCKING": "1",
//             },
//             "args": [
//                 "--nproc_per_node=3",
//                 "--nnodes=1",
//                 "--node_rank=0",
//                 "--master_addr=127.0.0.1",
//                 "--master_port=29500",
//                 "llava/train/train_mem.py",
//                 "--deepspeed", "scripts/zero3.json",
//                 "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
//                 "--version", "qwen_1_5",
//                 "--data_path", "scripts/train/onevision.yaml",
//                 // "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data",
//                 "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data/geo3k/",
//                 "--video_folder", "/mediaPFM/data/haozhe/onevision/llava_video",
//                 // "--video_folder", "/home/haozhe/kitchen/AVION/datasets",
//                 "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
//                 "--mm_vision_tower_lr", "2e-6",
//                 "--vision_tower", "google/siglip-so400m-patch14-384",
//                 "--mm_projector_type", "mlp2x_gelu",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end", "False",
//                 "--mm_use_im_patch_token", "False",
//                 "--group_by_modality_length", "True",
//                 "--image_aspect_ratio", "anyres_max_9",
//                 "--image_grid_pinpoints", "(1x1),...,(6x6)",
//                 "--mm_patch_merge_type", "spatial_unpad",
//                 "--bf16", "True",
//                 "--run_name", "test1",
//                 "--output_dir", "experiments/test1",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "1",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "2",
//                 "--evaluation_strategy", "steps",
//                 "--eval_steps", "100",
//                 "--save_strategy", "steps",
//                 "--save_steps", "2000",
//                 // "--save_total_limit", "1",
//                 "--learning_rate", "1e-5",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "32768",
//                 "--gradient_checkpointing", "True",
//                 "--dataloader_num_workers", "4",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "wandb",
//                 "--torch_compile", "True",
//                 "--torch_compile_backend", "inductor",
//                 "--dataloader_drop_last", "True",
//                 "--frames_upbound", "16",
//                 "--root", "/mediaPFM/data/haozhe/onevision/llava_video/EK100",
//                 "--action_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_val.json",
//                 "--val_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                 "--llava_num_frames", "16",
//                 "--clip_length", "16",
//                 "--action_representation", "GT_random_narration",
//                 "--topk_predictions", "5",
//                 "--dataset", "ek100_cls",
//                 "--vision_supervision", "three_tokens",
//                 "--action_types", "97,300,3806"
//             ],
//             "console": "integratedTerminal",
//             "justMyCode": false,
//             "cwd": "${workspaceFolder}"
//         }
//     ]
// }


// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "docs/LLaVA_OneVision_Tutorials.py",
//             "console": "integratedTerminal",
//             "env":{
//                 "CUDA_VISIBLE_DEVICES":"0,1,2,3",
//                 "HF_HOME": "huggingface",
//                 // "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7"
//                 },
//             "justMyCode": false,
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "docs/LLaVA_OneVision_debug.py",
//             "console": "integratedTerminal",
//             "env":{
//                 "CUDA_VISIBLE_DEVICES":"0,1,2,3",
//                 "HF_HOME": "huggingface",
//                 // "LD_PRELOAD": "/usr/lib/x86_64-linux-gnu/libffi.so.7"
//                 },
//             "args": [
//                 "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
//                 "--version", "qwen_1_5",
//                 "--data_path", "scripts/train/onevision.yaml",
//                 // "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data",
//                 "--image_folder", "/mediaPFM/data/haozhe/onevision/llava_data/geo3k/",
//                 "--video_folder", "/mediaPFM/data/haozhe/onevision/llava_video",
//                 // "--video_folder", "/home/haozhe/kitchen/AVION/datasets",
//                 "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
//                 "--mm_vision_tower_lr", "2e-6",
//                 "--vision_tower", "google/siglip-so400m-patch14-384",
//                 "--mm_projector_type", "mlp2x_gelu",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end", "False",
//                 "--mm_use_im_patch_token", "False",
//                 "--group_by_modality_length", "True",
//                 "--image_aspect_ratio", "anyres_max_9",
//                 "--image_grid_pinpoints", "(1x1),...,(6x6)",
//                 "--mm_patch_merge_type", "spatial_unpad",
//                 "--bf16", "True",
//                 "--run_name", "test1",
//                 "--output_dir", "experiments/test1",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "1",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "2",
//                 "--evaluation_strategy", "steps",
//                 "--eval_steps", "10",
//                 "--save_strategy", "steps",
//                 "--save_steps", "2000",
//                 // "--save_total_limit", "1",
//                 "--learning_rate", "1e-5",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "32768",
//                 "--gradient_checkpointing", "True",
//                 "--dataloader_num_workers", "4",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "wandb",
//                 "--torch_compile", "True",
//                 "--torch_compile_backend", "inductor",
//                 "--dataloader_drop_last", "True",
//                 "--frames_upbound", "16",
//                 "--root", "/mediaPFM/data/haozhe/onevision/llava_video/EK100",
//                 "--action_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_val.json",
//                 "--val_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                 "--llava_num_frames", "16",
//                 "--clip_length", "16",
//                 "--action_representation", "GT_random_narration",
//                 "--topk_predictions", "5",
//                 "--dataset", "ek100_cls",
//                 "--vision_supervision", "newline",
//                 "--action_types", "97,300,3806"
//             ],
//             "justMyCode": false,
//         }
//     ]
// }

// {
//     // Use IntelliSense to learn about possible attributes.
//     // Hover to view descriptions of existing attributes.
//     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "debugpy",
//             "request": "launch",
//             "program": "llava/action/generate_description.py",
//             "console": "integratedTerminal",
//             "env":{"CUDA_VISIBLE_DEVICES":"0"},
//             "justMyCode": false,
//             "args": [
//                 "--train_metadata", "/mediaPFM/data/haozhe/EK100/epic-kitchens-100-annotations/EPIC_100_train.csv",
//                 "--out_folder", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA",
//                 "--train_predictions", "/mediaPFM/data/haozhe/EK100/EK100_in_LLAVA/avion_pred_ids_train.json",
//                 "--gen_type", "avion_mc",
//                 "--n_options", "5",
//                 "--action_representation", "GT_key", //"GT_random_narration",
//                 "--n_narrations", "-1",
//             ]
//         }
//     ]
// }

//shaokai
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Run LLAVA Training with torchrun",
            "type": "debugpy",
            "request": "launch",
            "module": "torch.distributed.run",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "OMP_NUM_THREADS": "8",
                "NCCL_IB_DISABLE": "0",
                "NCCL_IB_GID_INDEX": "3",
                "NCCL_SOCKET_IFNAME": "eth0",
                "HF_HOME": "/data/shaokai",
                "NCCL_DEBUG": "INFO",
                "ACCELERATE_CPU_AFFINITY": "1",
                "WANDB_API_KEY": "4474ec79de023b0c3ffb43588ab6163264f875db",
                "PYTHONPATH": "/data/shaokai/LLaVA-NeXT:/usr/local/lib/python3.10/site-packages/decord-0.6.0-py3.10-linux-x86_64.egg/"
            },
            "args": [
                "--nproc_per_node=1",
                "--nnodes=1",
                "--node_rank=0",
                "--master_addr=127.0.0.1",
                "--master_port=29500",
                "llava/train/train_mem.py",                
                "--deepspeed", "scripts/zero3.json",
                "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
                "--version", "qwen_1_5",
                "--data_path", "scripts/train/simple_random_top5.yaml",
                "--video_folder", "/data/shaokai/",
                "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
                "--mm_vision_tower_lr", "2e-6",
                "--vision_tower", "google/siglip-so400m-patch14-384",
                "--mm_projector_type", "mlp2x_gelu",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                "--group_by_modality_length", "True",
                "--image_aspect_ratio", "anyres_max_9",
                "--image_grid_pinpoints", "(1x1),...,(6x6)",
                "--mm_patch_merge_type", "spatial_unpad",
                "--bf16", "True",
                "--run_name", "dpo_test",
                "--output_dir", "experiments/dpo_test",
                "--num_train_epochs", "1",
                "--per_device_train_batch_size", "1",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps", "2",
                "--evaluation_strategy", "steps",
                "--save_strategy", "steps",
                "--save_steps", "1000",
                "--save_total_limit", "1",
                "--learning_rate", "1e-5",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                "--tf32", "True",
                "--model_max_length", "32768",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "4",
                "--lazy_preprocess", "True",
                "--report_to", "wandb",
                "--torch_compile", "True",
                "--torch_compile_backend", "inductor",
                "--dataloader_drop_last", "True",
                "--frames_upbound", "4",
                "--root", "/data/shaokai/EK100",
                "--action_predictions", "/data/shaokai/AVION_PREDS/avion_pred_ids_val.json",
                "--val_metadata", "/data/shaokai/epic-kitchens-100-annotations/EPIC_100_validation.csv",
                "--llava_num_frames", "4",
                "--clip_length", "4",
                "--action_representation", "official_key",
                "--topk_predictions", "5",
                "--eval_steps", "1",
                "--vision_supervision", "one_token",
                "--vision_token_training", "all_layers",
                "--action_types",  "97,300,3806"
            ],
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}"
        }
    ]
}


// {
//         "version": "0.2.0",
//         "configurations": [
//             {
//                 "name": "Run LLAVA Training with torchrun",
//                 "type": "debugpy",
//                 "request": "launch",
//                 "module": "torch.distributed.run",
//                 "env": {
//                     "CUDA_VISIBLE_DEVICES": "0",
//                     "OMP_NUM_THREADS": "8",
//                     "NCCL_IB_DISABLE": "0",
//                     "NCCL_IB_GID_INDEX": "3",
//                     "NCCL_SOCKET_IFNAME": "eth0",
//                     "HF_HOME": "/data/shaokai",
//                     "NCCL_DEBUG": "INFO",
//                     "ACCELERATE_CPU_AFFINITY": "1",
//                     "WANDB_API_KEY": "4474ec79de023b0c3ffb43588ab6163264f875db",
//                     "PYTHONPATH": "/data/shaokai/LLaVA-NeXT:/usr/local/lib/python3.10/site-packages/decord-0.6.0-py3.10-linux-x86_64.egg/"
//                 },
//                 "args": [
//                     "--nproc_per_node=1",
//                     "--nnodes=1",
//                     "--node_rank=0",
//                     "--master_addr=127.0.0.1",
//                     "--master_port=29500",
//                     "llava/action/ek_eval.py",                
//                     "--pretrained_name", "llava-onevision-qwen2-0.5b-ov",
//                     "--root", "/data/shaokai/EK100",
//                     "--train-metadata", "/data/shaokai/epic-kitchens-100-annotations/EPIC_100_train.csv",
//                     "--val-metadata", "/data/shaokai/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                     "--llava_num_frames", "12",
//                     "--clip-length", "12",
//                     "--action_predictions","/data/shaokai/TIM_PREDS/tim_pred_ids_val.json",
//                     "--action_representation", "gpt_narration",
//                     //"--llava_checkpoint", "/data/shaokai/LLaVA-NeXT/experiments/dev_0.5b_12f_avion_top5_pad_with_neighbors",
//                     "--topk_predictions", "5",
//                     //"--learn_neighbor_actions"
//                 ],
//                 "console": "integratedTerminal",
//                 "justMyCode": false,
//                 "cwd": "${workspaceFolder}"
//             }
//         ]
// }


// {
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Run LLAVA Training with torchrun",
//             "type": "debugpy",
//             "request": "launch",
//             "module": "torch.distributed.run",
//             "env": {
//                 "CUDA_VISIBLE_DEVICES": "0",
//                 "OMP_NUM_THREADS": "8",
//                 "NCCL_IB_DISABLE": "0",
//                 "NCCL_IB_GID_INDEX": "3",
//                 "NCCL_SOCKET_IFNAME": "eth0",
//                 "HF_HOME": "/data/shaokai",
//                 "NCCL_DEBUG": "INFO",
//                 "ACCELERATE_CPU_AFFINITY": "1",
//                 "WANDB_API_KEY": "4474ec79de023b0c3ffb43588ab6163264f875db",
//                 "PYTHONPATH": "/data/shaokai/LLaVA-NeXT:/usr/local/lib/python3.10/site-packages/decord-0.6.0-py3.10-linux-x86_64.egg/"
//             },
//             "args": [
//                 "--nproc_per_node=1",
//                 "--nnodes=1",
//                 "--node_rank=0",
//                 "--master_addr=127.0.0.1",
//                 "--master_port=29500",
//                 "llava/train/train_dpo_new.py",                
//                 "--deepspeed", "scripts/zero3.json",
//                  "--model_name_or_path", "lmms-lab/llava-onevision-qwen2-0.5b-ov",
//                 "--dpo_alpha", "1.0",
//                 "--beta=0.1",
//                 "--gamma", "0",
//                 "--version", "qwen_1_5",
//                 "--data_path", "scripts/train/simple_tim_top5_cut.yaml",
//                 "--video_folder", "/data/shaokai/",
//                 "--mm_tunable_parts", "mm_vision_tower,mm_mlp_adapter,mm_language_model",
//                 "--mm_vision_tower_lr", "2e-6",
//                 "--vision_tower", "google/siglip-so400m-patch14-384",
//                 "--mm_projector_type", "mlp2x_gelu",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end", "False",
//                 "--mm_use_im_patch_token", "False",
//                 "--group_by_modality_length", "True",
//                 "--image_aspect_ratio", "anyres_max_9",
//                 "--image_grid_pinpoints", "(1x1),...,(6x6)",
//                 "--mm_patch_merge_type", "spatial_unpad",
//                 "--bf16", "True",
//                 "--run_name", "dpo_test",
//                 "--output_dir", "experiments/dpo_test",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "1",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "2",
//                 "--evaluation_strategy", "steps",
//                 "--save_strategy", "steps",
//                 "--save_steps", "1000",
//                 "--save_total_limit", "1",
//                 "--learning_rate", "1e-5",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "32768",
//                 "--gradient_checkpointing", "True",
//                 "--dataloader_num_workers", "4",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "wandb",
//                 "--torch_compile", "True",
//                 "--torch_compile_backend", "inductor",
//                 "--dataloader_drop_last", "True",
//                 "--frames_upbound", "32",
//                 "--root", "/data/shaokai/EK100",
//                 "--action_predictions", "/data/shaokai/TIM_PREDS/tim_pred_ids_val.json",
//                 "--val_metadata", "/data/shaokai/epic-kitchens-100-annotations/EPIC_100_validation.csv",
//                 "--llava_num_frames", "16",
//                 "--clip_length", "16",
//                 "--action_representation", "GT_random_narration",
//                 "--topk_predictions", "5",
//                 "--eval_steps", "10",

//             ],
//             "console": "integratedTerminal",
//             "justMyCode": false,
//             "cwd": "${workspaceFolder}"
//         }
//     ]
// }
