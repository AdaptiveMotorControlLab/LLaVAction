# LLaVAction: Evaluating and Training Multi-Modal Large Language Models for Action Recognition


- This repository contains the implementation for our ICCV 2025 submission on evaluating and training multi-modal large language models for action recognition. 
- Our code is built on [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT), and files in the directory `llavaction/action` are related to this work. We thank the authors of LLaVA-NeXT for making their code publicly available.
- The files in the `\eval`, `\model`, `\serve` and `\train` are directly from [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT), unless modified and noted below.
- Modified files are:
  - - file path
  - - 
A diff can be generated against the FIX of LLaVA-NeXT to see our modifications.
- The code will be made publicly available when published. For review, the provided code and model license is [no license](https://choosealicense.com/no-permission/).
- Please see the `\example` directory for a demo notebook and installation instructions.
