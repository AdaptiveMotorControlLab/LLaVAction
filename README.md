# LLaVAction: Evaluating and Training Multi-Modal Large Language Models for Action Recognition


- This repository contains the implementation for our ICCV 2025 submission on evaluating and training multi-modal large language models for action recognition. 
- Our code is built on [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT), and files in the directory `llava/action` are related to this work. We thank the authors of LLaVA-NeXT for making their code publicly available.
A diff can be generated against the FIX of LLaVA-NeXT to see our modifications.
- The code will be open sourced when published. For review, the license is [no license](https://choosealicense.com/no-permission/).
- Please see the `\example` directory for a demo notebook and installation instructions.
